# -*- coding: utf-8 -*-
"""CSE422 PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ogi1qAxvW1P5PhEhJs3WQSrOgCDDJHtW

**<h3>Libaries</h3>**
1. Importing ***pandas*** for reading the dataset and operating on it.<br>
2. Importing ***matplotlib.pyplot*** and ***seaborn*** for data visualization.<br>
3. Importing ***confusion_matrix*** and ***classification_report*** from ***sklearn.metrics*** for showing **Confusion Matrix, Accuracy, Precesion, Recall, F1 Score**.<br>
4. Importing ***plot_confusion_matrix*** to visualize the confusion matrix from ***mlxtend.plotting***.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from sklearn.metrics import confusion_matrix, classification_report
from mlxtend.plotting import plot_confusion_matrix

"""**Reading the dataset**<br>
Here, we are using ***read_csv*** module of ***pandas*** to read the csv formatted dataset named *cardio_train.csv* from Google Drive.
"""

from google.colab import files

uploaded = files.upload()

data = pd.read_csv('cardio_train.csv', sep = ';')
data

"""**<h3>Contents of the dataset</h3>**
There are 13 columns and 70000 rows in this dataset.
1. ***id***: This is unique for all the data.
2. ***age***: Here, age is given in days.
3. ***gender***:
    Women = 1 and
    Men = 2
4. ***height***: Here, height is given in cm.
5. ***weight***: Here, weight is given in kg.
6. ***ap_hi***: Systolic blood pressure
7. ***ap_lo***: Diastolic blood pressure
8. ***cholesterol***:
    Normal = 1,
    Above normal = 2 and
    Well above normal = 3
9. ***gluc***:
    Normal = 1,
    Above normal = 2 and
    Well above normal = 3
10. ***smoke***: Smoker or not. Binary.
11. ***alco***: Drink or not. Binary.
12. ***active***: Does physical excercise or not. Binary.
13. ***cardio*** [Target]: Has heart disease or not. Binary.

# Preparing the dataset.

1. Here the unique ID for all the data is not needed. So we can delete than column.
2. The ***'age'*** is in **days**. Converting to **year** will give us some upperhand.
"""

#deleting 'id'
del data["id"]

#converting the 'age' from day to year
data["age"] //= 365

"""Now we need to check if the there is any null value in the dataset."""

data.isnull().values.any() #Gives boolean value as output (i.e. True or False)

data.isna().sum() #Detailed check for null value

"""As we can see there is no null value in the dataset. So, now we can process further.

# Quality check of the dataset.
"""

values = data['cardio'].value_counts()

values

#Visualizing
seaborn.set(rc={'figure.figsize':(11.7,8.9)})
seaborn.countplot(x = data['cardio'])

"""We can see that the dataset has quite balanced data, where almost half of the population have cardiovascular diseases and half of them don't.<br>
**Comment:** QC Passed

# Data visualization
Now we will see the hue of cardiovascular diseases in terms of age, height, weight, smoke, and so on.<br>
This visualization will give us some general overview about the nature of the data which we are working with.<br>
**Nerd info:**<br>
We will use *sklearn* and *matplotlib.pyplot*.<br>
Here,
1. x-axis will be variable
2. hue = "cardio"
3. data = data (dataset we are working with)
4. palette = "colorblind"
5. edgecolor = "dark"
<br><br>
✔ $\color{#c38820}{\text{This color means, the data has cardiovascular disease.}}$<br>
✖ $\color{#176d9c}{\text{This color means, the data doesn't have cardiovascular disease.}}$
"""

seaborn.set(rc={'figure.figsize':(12,9)}) #setting the figure size

"""**<h4>Age</h4>**
Here we can see that, heart diseases increase with age. When a person is in his 60s, there is a high chance of getting cardiovascular diseases.
"""

seaborn.countplot(x = "age", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""**<h4>Gender</h4>**
Here we can see that, our dataset has a greater number of women than men, and the ratio of cardio 0:1, is almost 50-50. Which concludes that, CVDs (cardiovascular diseases) are almost gender neutral.

"""

seaborn.countplot(x = "gender", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""**<h4>BMI</h4>**
As we already know, we don't have any attributes for BMI. But we have the attributes to calculate the BMI.
$$
B M I=\frac{\text { weight }(\mathrm{kg})}{\text { height }^{2}\left(\mathrm{~m}^{2}\right)}
$$
From this equation, we can easily calculate the BMI of every population in the dataset by using height and weight attributes.<br><br>
**Note:** In our dataset, the height is given in centimeter. We will convert it to meter by dividing by 100.<br>
<center><img src = 'https://files.prokerala.com/health/images/bmi-category.png'></center>
"""

data['bmi'] = data['weight']/((data['height']/100)**2)

pd.options.mode.chained_assignment = None #To removing SettingWithCopyWarning in pandas

for i in range(len(data['bmi'])):
    if 0 < data['bmi'][i] < 18.5:
        data['bmi'][i] = 'Underwight'
    elif 18.5 < data['bmi'][i] < 25:
        data['bmi'][i] = 'Normal / Optimal'
    elif 25 < data['bmi'][i] < 30:
        data['bmi'][i] = 'Overweight'
    else:
        data['bmi'][i] = 'Obese'

seaborn.countplot(x = "bmi", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""Here the observation is, the more risk of getting CVDs is when a person is Obese, and Overweight.

**<h4>Glucose</h4>**
We have less data on glucose level 2 and 3. But in those data, the cardio 0:1 ratio is much higher towards cardio: 1.<br>
This concludes that, higher the glucose level, the higher the chances of having CVDs.
"""

seaborn.countplot(x = "gluc", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""**<h4>Cholesterol</h4>**
We also have less data on cholestorel level 2 and 3. But in those data, the cardio 0:1 ratio is much higher towards cardio: 1.<br>
This concludes that, higher the cholesterol level, the higher the chances of having CVDs.
"""

seaborn.countplot(x = "cholesterol", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""**<h4>Physical Excercise</h4>**
People who don't workout have higher cardio ratio than of those who workout.<br>
0 = Do not workout<br>
1 = Do workout
"""

seaborn.countplot(x = "active", hue = "cardio", data = data, palette = 'colorblind', edgecolor = seaborn.color_palette('dark', n_colors = 1))

"""Here is a summary of BMI and Age relating to CVDs, showing in boxplot and kdeplot."""

fig, ax = plt.subplots(2,2,figsize = (20, 15))
data['bmi'] = data['weight']/((data['height']/100)**2)
seaborn.boxplot(data = data, x='bmi', ax=ax[0,0])
seaborn.boxplot(data = data, x='age', ax=ax[0,1])

seaborn.kdeplot(x = data['bmi'], hue= data['cardio'], ax= ax[1,0])
seaborn.kdeplot(x = data['age'], hue= data['cardio'], ax= ax[1,1])
plt.show()

"""Now that we have visualized many of the data, we can restore the dataset to default attributes by deleting 'bmi' to work on it."""

del data["bmi"]

"""<h4>Here is the correlation of between the attributes."""

data.corr()

"""<h4>Here is the heatmap from the correlation."""

seaborn.heatmap(data.corr(), annot = True)

"""<h4>Here is the mathematical brief description of the attributes."""

data.describe()

"""# Splitting the dataset for train and test
We are using ***iloc*** from *pandas* for general split.<br>
Than, we are going to import ***train_test_split*** from ***sklearn.model_selection***. And going to use this module to split the data into **70%** of train data naming, *xtrain* and *ytrain* and **30%** of test data naming *xtest*, *ytest*.
"""

x = data.iloc[:,:-1]; x

y = data.iloc[:, -1]; y

from sklearn.model_selection import train_test_split

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.3, random_state = 1)

print("Size of train data:", xtrain.shape)
print("Size of test data:", xtest.shape)

"""Here 70% of 70,000 are 49,000<br>
and 30% of 70,000 are 21,000<br><br>
**Comment:** Splitting succesful!

#Training and Testing
Now that we are finally ready to train and test our data using some algoritms i.e. Random Forest, Logistic Regression etc.<br><br>
We are going create an object for all the imported algorithm classifiers form ***sklearn***, and than we are going to call the ***fit*** function for the object to train the data and than we are going to test it.

**<h3>1. Random Forest</h3>**

**Algorithm brief:** Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.<br>

**After running the algoritm:**
*   Accuracy for the train data is: **98%**
*   Accuracy for the test data is: **71%**
"""

from sklearn.ensemble import RandomForestClassifier #importing the algorithm

rfc_object = RandomForestClassifier() #creating an object under it.

rfc_object.fit(xtrain, ytrain) #training the dataset

train_score = rfc_object.score(xtrain, ytrain)
test_score = rfc_object.score(xtest, ytest)

print(f"The taining accuracy of the model is: {train_score}")
print(f"The test accuracy of the model is: {test_score}")

"""**Confusion Matrix and Performance calculation:**
* Confusion matrix (Color Blue)
* Accuracy
* Precision
* Recall
* F1 Score
"""

pred = rfc_object.predict(xtest)
cm = confusion_matrix(ytest, pred)
print("Confusion matrix:", cm, sep = '\n')
print()
print("Visualizing:")
fig, ax = plot_confusion_matrix(conf_mat = cm, show_absolute = True, show_normed = True, colorbar = True, cmap = "Blues")
plt.show()
print()
print("Classification Report:", classification_report(ytest, pred), sep = '\n')

"""**<h3>2. Logistic Regression</h3>**

**Algorithm brief:** Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.<br>

**After running the algoritm:**
*   Accuracy for the train data is: **72%**
*   Accuracy for the test data is: **72%**
"""

from sklearn.linear_model import LogisticRegression #importing the algorithm

lr_object = LogisticRegression(solver='lbfgs', max_iter=1000) #creating an object under it.

lr_object.fit(xtrain, ytrain) #training the dataset

train_score = lr_object.score(xtrain, ytrain)
test_score = lr_object.score(xtest, ytest)

print(f"The taining accuracy of the model is: {train_score}")
print(f"The test accuracy of the model is: {test_score}")

"""**Confusion Matrix and Performance calculation:**
* Confusion matrix (Color Blue)
* Accuracy
* Precision
* Recall
* F1 Score
"""

pred = lr_object.predict(xtest)
cm = confusion_matrix(ytest, pred)
print("Confusion matrix:", cm, sep = '\n')
print()
print("Visualizing:")
fig, ax = plot_confusion_matrix(conf_mat = cm, show_absolute = True, show_normed = True, colorbar = True, cmap = "Blues")
plt.show()
print()
print("Classification Report:", classification_report(ytest, pred), sep = '\n')

"""**<h3>3. Multi-layer Perceptron</h3>**

**Algorithm brief:** A multilayer perceptron is a fully connected class of feedforward artificial neural network. The term MLP is used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons.<br>

**After running the algoritm:**
*   Accuracy for the train data is: **72%**
*   Accuracy for the test data is: **72.6%**


"""

from sklearn.neural_network import MLPClassifier #importing the algorithm

MLP_object = MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=1000) #creating an object under it.

MLP_object.fit(xtrain, ytrain) #training the dataset

train_score = MLP_object.score(xtrain, ytrain)
test_score = MLP_object.score(xtest, ytest)

print(f"The taining accuracy of the model is: {train_score}")
print(f"The test accuracy of the model is: {test_score}")

"""**Confusion Matrix and Performance calculation:**
* Confusion matrix (Color Blue)
* Accuracy
* Precision
* Recall
* F1 Score
"""

pred = MLP_object.predict(xtest)
cm = confusion_matrix(ytest, pred)
print("Confusion matrix:", cm, sep = '\n')
print()
print("Visualizing:")
fig, ax = plot_confusion_matrix(conf_mat = cm, show_absolute = True, show_normed = True, colorbar = True, cmap = "Blues")
plt.show()
print()
print("Classification Report:", classification_report(ytest, pred), sep = '\n')

"""**<h3>4. K-Nearest Neighbor</h3>**

**Algorithm brief:** The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.<br>

**After running the algoritm:**
*   Accuracy for the train data is: **75%**
*   Accuracy for the test data is: **71%**


"""

from sklearn.neighbors import KNeighborsClassifier #importing the algorithm

knn = KNeighborsClassifier(n_neighbors = 11) #creating an object under it.

knn.fit(xtrain, ytrain) #training the dataset

train_score = knn.score(xtrain, ytrain)
test_score = knn.score(xtest, ytest)

print(f"The taining accuracy of the model is: {train_score}")
print(f"The test accuracy of the model is: {test_score}")

"""**Confusion Matrix and Performance calculation:**
* Confusion matrix (Color Blue)
* Accuracy
* Precision
* Recall
* F1 Score
"""

pred = knn.predict(xtest)
cm = confusion_matrix(ytest, pred)
print("Confusion matrix:", cm, sep = '\n')
print()
print("Visualizing:")
fig, ax = plot_confusion_matrix(conf_mat = cm, show_absolute = True, show_normed = True, colorbar = True, cmap = "Blues")
plt.show()
print()
print("Classification Report:", classification_report(ytest, pred), sep = '\n')

"""**<h3>5. Support Vector Machine: C-support Vector Classification</h3>**

**Algorithm brief:** SVC is a nonparametric clustering algorithm that does not make any assumption on the number or shape of the clusters in the data. It is known that, this algorithm works best for low-dimensional data, so if the data is high-dimensional, a preprocessing step, e.g. using principal component analysis, is usually required.<br>

**After running the algoritm:**
*   Accuracy for the train data is: **72%**
*   Accuracy for the test data is: **72%**


"""

from sklearn.svm import SVC #importing the algorithm

svc_object = SVC() #creating an object under it.

svc_object.fit(xtrain, ytrain) #training the dataset

train_score = svc_object.score(xtrain, ytrain)
test_score = svc_object.score(xtest, ytest)

print(f"The taining accuracy of the model is: {train_score}")
print(f"The test accuracy of the model is: {test_score}")

"""**Confusion Matrix and Performance calculation:**
* Confusion matrix (Color Blue)
* Accuracy
* Precision
* Recall
* F1 Score
"""

pred = svc_object.predict(xtest)
cm = confusion_matrix(ytest, pred)
print("Confusion matrix:", cm, sep = '\n')
print()
print("Visualizing:")
fig, ax = plot_confusion_matrix(conf_mat = cm, show_absolute = True, show_normed = True, colorbar = True, cmap = "Blues")
plt.show()
print()
print("Classification Report:", classification_report(ytest, pred), sep = '\n')